\documentclass[a4paper]{scrartcl}  

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

\usepackage[margin=.75in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{color}
\usepackage{graphicx}
\usepackage{hyperref}

\title{Github Repository Classification}

\begin{document}

\maketitle
\section{Abstract}
TODO
\section{Introduction}
TODO

\section{Metrics}
In this section we present the metrics we use for repository classification. We extract them using the Github API and the cloned repository.

scale features (logarithm)
\begin{description}
	
	\item[DOC terms in readme]
	Returns the number of common DOC terms in the Readme file. Under common DOC terms we understand: 'documentation', 'usage', 'guide', 'installation', 'getting started', 'quickstart', 'tutorial' and 'setup'.
	
	TODO: boxplot + analysis
	\item[File count]
	This is the number files in the repository excluding the .git directory.
	
	TODO: boxplot + analysis
	Additionally we calculate the ratio of specific file types to the total file count. There are metrics for each of the following file types: pdf, html, png, latex, source code and markdown. We identify these file types by the file extension.
	TODO: no boxplot but why these types?
	\item[Forks count]
	This metric contains the number of forks for a repository.
	TODO: boxplot + analysis
	\item[File to folder ratio]
	This calculates the ratio between number of files and directories. We expect the DOCS and some DEV (e.g. java programs) to have a low file-folder-ratio.
	TODO: boxplot
	\item[Open issue count]
	The number of currently open issues. Many open issues indicate a popular repository. We expect mostly DEV to have a high open issue count.
	TODO: boxplot
	\item[Average entropy]
	The entropy of each repository file is calculated and the average of all these files is returned. We expect DATA and EDU to have a high entropy because of many pictures and videos. DOCS and WEB have a medium entropy because of some pictures. DEV and HW have a low entropy beause they contain mostly source code.
	TODO: boxplot
	\item[Edu mail ratio]
	This is the ration between university mail addresses to all unique addresses that appear in a repository. We extract them from the commit messages. We expect EDU to have a high Edu mail ratio.
	TODO: boxplot 
	\item[HW terminology in commits]
	We search all commit messages for common HW terms and return the number of occurrences. Under common HW terms we understand: 'exercise', 'assignment', 'question', 'task', 'homework', 'student' and 'solution' 
	\item[Repository size]
	The repository size in kilobytes. Repositories of each category can have a small size but we only expect DEV, DOCS, DATA and WEB repository to have a big size.
	\item[Average folder depth]
	\item[Up-to-dateness]
	Measures the time since the last commit. We expect HW repositories to have a low up-to-dateness because they are not worked on after deadline.
	\item[Is github.io page]
	Github host websites via Github pages. There are dedicated repositories with the name scheme <username>.github.io. This metrics helps us to identify such repositories as WEB.
	TODO: box plot
	\item[HW terminology in filenames or directory names]
	Like HW terminology in commits but we look for the terms in file names.
	
	\item[Is 'doc' in repository title or description]
	We search for the term 'docs', 'documents' or 'documentation' in the repository title or description. This is a strong indicator if it is a DOC repository.
	TODO: blox
	\item[Watcher count]
	The watcher count indicates the popularity of a repository. We expect only DEV repositories to have a high watcher count. EDU could also have an higher than average watcher count.
	TODO: blox
	\item[Intro or course in title or description]
	This metric searches for the terms 'intro' and 'course' in the repository title and description. If that is the case that is a strong indicator for a EDU repository.
	TODO: boxplot
	
\end{description}
	

\section{Validation}


\section{What we have done}


\section{Program usage}
The entry point of the program is main.py. Running \textit{python3 main.py} starts the test mode. This mode trains and validates different models. When given a file as parameter (e.g. \textit{python3 main.py data/valset\_unclassified.txt}), the program classifies all repositories in that file and print the results to stdout.

Saving the trained model with pickle or joblib resulted a strange loss in accuracy when loading it again. That is why we fall to the solution of training the model at the start of the prediction phase.


\end{document}
