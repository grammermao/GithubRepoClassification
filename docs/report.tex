\documentclass[a4paper]{scrartcl}  

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

\usepackage[margin=.75in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{color}
\usepackage{graphicx}
\usepackage{hyperref}

\title{Github Repository Classification}

\begin{document}

\maketitle
\section{Abstract}
TODO
\section{Introduction}
TODO

\section{Metrics}
In this section we present the metrics we use for repository classification. We extract them using the Github API and the cloned repository.

scale features (logarithm)
\begin{description}
	
	\item[DOC terms in readme]
	Returns the number of common DOC terms in the Readme file. Under common DOC terms we understand: 'documentation', 'usage', 'guide', 'installation', 'getting started', 'quickstart', 'tutorial' and 'setup'.
	
	TODO: boxplot + analysis
	\item[File count]
	This is the number files in the repository excluding the .git directory.
	
	TODO: boxplot + analysis
	Additionally we calculate the ratio of specific file types to the total file count. There are metrics for each of the following file types: pdf, html, png, latex, source code and markdown. We identify these file types by the file extension.
	TODO: no boxplot but why these types?
	\item[Forks count]
	This metric contains the number of forks for a repository.
	TODO: boxplot + analysis
	\item[File to folder ratio]
	This calculates the ratio between number of files and directories. We expect the DOCS and some DEV (e.g. java programs) to have a low file-folder-ratio.
	TODO: boxplot
	\item[Open issue count]
	The number of currently open issues. Many open issues indicate a popular repository. We expect mostly DEV to have a high open issue count.
	TODO: boxplot
	\item[Average entropy]
	The entropy of each repository file is calculated and the average of all these files is returned. We expect DATA and EDU to have a high entropy because of many pictures and videos. DOCS and WEB have a medium entropy because of some pictures. DEV and HW have a low entropy beause they contain mostly source code.
	TODO: boxplot
	\item[Edu mail ratio]
	This is the ration between university mail addresses to all unique addresses that appear in a repository. We extract them from the commit messages. We expect EDU to have a high Edu mail ratio.
	TODO: boxplot 
	\item[HW terminology in commits]
	We search all commit messages for common HW terms and return the number of occurrences. Under common HW terms we understand: 'exercise', 'assignment', 'question', 'task', 'homework', 'student' and 'solution' 
	\item[Repository size]
	The repository size in kilobytes. Repositories of each category can have a small size but we only expect DEV, DOCS, DATA and WEB repository to have a big size.
	\item[Average folder depth]
	\item[Up-to-dateness]
	Measures the time since the last commit. We expect HW repositories to have a low up-to-dateness because they are not worked on after deadline.
	\item[Is github.io page]
	Github host websites via Github pages. There are dedicated repositories with the name scheme <username>.github.io. This metrics helps us to identify such repositories as WEB.
	TODO: box plot
	\item[HW terminology in filenames or directory names]
	Like HW terminology in commits but we look for the terms in file names.
	
	\item[Is 'doc' in repository title or description]
	We search for the term 'docs', 'documents' or 'documentation' in the repository title or description. This is a strong indicator if it is a DOC repository.
	TODO: blox
	\item[Watcher count]
	The watcher count indicates the popularity of a repository. We expect only DEV repositories to have a high watcher count. EDU could also have an higher than average watcher count.
	TODO: blox
	\item[Intro or course in title or description]
	This metric searches for the terms 'intro' and 'course' in the repository title and description. If that is the case that is a strong indicator for a EDU repository.
	TODO: boxplot
	
\end{description}
	

\section{Validation}

The table shows our manual prediction versus the predictions generated by our model. The accuracy of the automatic prediction is 45\%. Our accuracy on our full dataset is 70\%. The dataset contains of the given repositories from InformatiCup and some manual classified repositories. The complete list can be found in \textit{data/testset.csv}. We used a 3-fold cross-validation to calculate the score in order to use the limited dataset size most efficiently.

\begin{center}
	\begin{tabular}{ l | c | c }
		\hline
		Repository & manual pred & automatic pred \\ \hline
		https://github.com/ga-chicago/wdi5-homework & HW & HW \\ \hline
		https://github.com/Aggregates/MI\_HW2 & HW & HW \\ \hline
		https://github.com/datasciencelabs/2016/ & EDU & EDU \\ \hline
		https://github.com/githubteacher/intro-november-2015 & EDU & EDU \\ \hline
		https://github.com/atom/atom & DEV & DEV \\ \hline
		https://github.com/jmcglone/jmcglone.github.io & WEB & DEV \\ \hline
		https://github.com/hpi-swt2-exercise/java-tdd-challenge & EDU & DEV \\ \hline
		https://github.com/alphagov/performanceplatform-documentation & DOCS & DOCS \\ \hline
		https://github.com/harvesthq/how-to-walkabout & DOCS & DOCS \\ \hline
		https://github.com/vhf/free-programming-books & DATA & DATA \\ \hline
		https://github.com/d3/d3 & DEV & DATA \\ \hline
		https://github.com/carlosmn/CoMa-II & HW & DEV \\ \hline
		https://github.com/git/git-scm.com & WEB & DEV \\ \hline
		https://github.com/PowerDNS/pdns & DEV & DEV \\ \hline
		https://github.com/cmrberry/cs6300-git-practice & HW & OTHER \\ \hline
		https://github.com/Sefaria/Sefaria-Project & DEV & DEV \\ \hline
		https://github.com/mongodb/docs & DOCS & DEV \\ \hline
		https://github.com/sindresorhus/eslint-config-xo & DEV & DATA \\ \hline
		https://github.com/e-books/backbone.en.douceur & DATA & DATA \\ \hline
		https://github.com/erikflowers/weather-icons & DEV & DATA \\ \hline
		https://github.com/tensorflow/tensorflow & DEV & DEV \\ \hline
		https://github.com/cs231n/cs231n.github.io & WEB & DEV \\ \hline
		https://github.com/m2mtech/smashtag-2015 & EDU & DEV \\ \hline
		https://github.com/openaddresses/openaddresses & DATA & DEV \\ \hline
		https://github.com/benbalter/congressional-districts & DATA & DATA \\ \hline
		https://github.com/Chicago/food-inspections-evaluation & DEV & DATA \\ \hline
		https://github.com/OpenInstitute/OpenDuka & WEB & DEV \\ \hline
		https://github.com/torvalds/linux & DEV & DEV \\ \hline
		https://github.com/bhuga/bhuga.net & WEB & DOCS \\ \hline
		https://github.com/macloo/just\_enough\_code & EDU & DEV \\ \hline
		https://github.com/hughperkins/howto-jenkins-ssl & EDU & DATA \\ \hline
	\end{tabular}
\end{center}

\subsection{Precision and Recall}

\begin{center}
	\begin{tabular}{ l | c | c }
		Category & Precision & Recall \\ \hline
		DEV & 5/9 &  \\ \hline
		HW & 2/4 & \\ \hline
		EDU & 2/6 & \\ \hline
		DOCS & 2/3 & \\ \hline
		WEB & 0/5 & \\ \hline
		DATA & 3/4 & \\ \hline
		OTHER & 0/0 & \\ \hline
	\end{tabular}
\end{center}

\section{Program usage}
The entry point of the program is main.py. Running \textit{python3 main.py} starts the test mode. This mode trains and validates different models. When given a file as parameter (e.g. \textit{python3 main.py data/valset\_unclassified.txt}), the program classifies all repositories in that file and print the results to stdout.

Saving the trained model with pickle or joblib resulted a strange loss in accuracy when loading it again. That is why we fall to the solution of training the model at the start of the prediction phase.


\end{document}
